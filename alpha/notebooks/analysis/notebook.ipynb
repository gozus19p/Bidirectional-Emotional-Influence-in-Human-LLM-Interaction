{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas pymongo openpyxl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "from pymongo.database import Database\n",
    "\n",
    "mongo_client: MongoClient = MongoClient(\"mongodb://localhost:27017/thesis\")\n",
    "thesis_database: Database = mongo_client[\"thesis\"]\n",
    "case_collection: Collection = thesis_database[\"Case\"]\n",
    "evaluation_collection: Collection = thesis_database[\"Evaluation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from logic.evaluation.model import Evaluation, Metrics\n",
    "\n",
    "\n",
    "evaluations: List[Evaluation] = [\n",
    "    Evaluation(\n",
    "        experiment=e[\"experiment\"],\n",
    "        model=e[\"model\"],\n",
    "        version=e[\"version\"],\n",
    "        date=e[\"date\"],\n",
    "        metrics=Metrics(**e[\"metrics\"])\n",
    "    )\n",
    "    for e in evaluation_collection.find()\n",
    "]\n",
    "len(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def normalize_evaluations(evaluations_: List[Evaluation]) -> List[Dict[str, Any]]:\n",
    "    normalized_evaluations: List[Dict[str, Any]] = []\n",
    "    for evaluation in evaluations_:\n",
    "        normalized_evaluations.append({\n",
    "            \"experiment\": evaluation.experiment,\n",
    "            \"model\": evaluation.model,\n",
    "            \"version\": evaluation.version,\n",
    "            \"date\": evaluation.date,\n",
    "            \"metric\": [k for k in evaluation.metrics.apathetic.keys()][0],\n",
    "            \"apathetic_score\": [v for v in evaluation.metrics.apathetic.values()][0],\n",
    "            \"joy_score\": [v for v in evaluation.metrics.joy.values()][0],\n",
    "            \"fear_score\": [v for v in evaluation.metrics.fear.values()][0],\n",
    "            \"anger_score\": [v for v in evaluation.metrics.anger.values()][0],\n",
    "        })\n",
    "    return normalized_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_evaluations: List[Dict[str, Any]] = normalize_evaluations(evaluations_=evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>model</th>\n",
       "      <th>version</th>\n",
       "      <th>date</th>\n",
       "      <th>metric</th>\n",
       "      <th>apathetic_score</th>\n",
       "      <th>joy_score</th>\n",
       "      <th>fear_score</th>\n",
       "      <th>anger_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BooleanQAExperiment</td>\n",
       "      <td>gemma2</td>\n",
       "      <td>9b-instruct-q4_0</td>\n",
       "      <td>2025-01-19 13:43:34</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BooleanQAExperiment</td>\n",
       "      <td>llama3.1</td>\n",
       "      <td>8b-instruct-q4_0</td>\n",
       "      <td>2025-01-19 13:43:34</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BooleanQAExperiment</td>\n",
       "      <td>llama3.2</td>\n",
       "      <td>3b-instruct-q4_0</td>\n",
       "      <td>2025-01-19 13:43:34</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BooleanQAExperiment</td>\n",
       "      <td>mistral</td>\n",
       "      <td>7b-instruct-q4_0</td>\n",
       "      <td>2025-01-19 13:43:34</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BooleanQAExperiment</td>\n",
       "      <td>qwen2.5</td>\n",
       "      <td>7b-instruct-q4_0</td>\n",
       "      <td>2025-01-19 13:43:34</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            experiment     model           version                 date  \\\n",
       "6  BooleanQAExperiment    gemma2  9b-instruct-q4_0  2025-01-19 13:43:34   \n",
       "2  BooleanQAExperiment  llama3.1  8b-instruct-q4_0  2025-01-19 13:43:34   \n",
       "8  BooleanQAExperiment  llama3.2  3b-instruct-q4_0  2025-01-19 13:43:34   \n",
       "0  BooleanQAExperiment   mistral  7b-instruct-q4_0  2025-01-19 13:43:34   \n",
       "4  BooleanQAExperiment   qwen2.5  7b-instruct-q4_0  2025-01-19 13:43:34   \n",
       "\n",
       "     metric  apathetic_score  joy_score  fear_score  anger_score  \n",
       "6  accuracy            0.882      0.874       0.872        0.866  \n",
       "2  accuracy            0.790      0.794       0.696        0.816  \n",
       "8  accuracy            0.478      0.686       0.640        0.670  \n",
       "0  accuracy            0.604      0.684       0.724        0.656  \n",
       "4  accuracy            0.810      0.784       0.730        0.784  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe: pd.DataFrame = pd.DataFrame(normalized_evaluations)\n",
    "dataframe.sort_values(by=[\"experiment\", \"model\", \"version\"], inplace=True)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_excel(\"analysis.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
